{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPOhB9nmW3b9v6k1/lINlMU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation"
      ],
      "metadata": {
        "id": "IKjDS0hi4vl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A recurrent neural network will be trained to generate text, character by character, inspired by CharRNN. The neural network will receive as input a sequence of letters and must output the next letter and so on. "
      ],
      "metadata": {
        "id": "xjjIMjTU4_v6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "22o3bfhQ5ff8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc_hmn0M5uPT",
        "outputId": "244b9d7e-3611-4c7f-eb34-a982b8ebc18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=8d84eed95a5891ff847a53d67c3e45f9fcecc69cc2dfa6315039117ea7f6d3d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "\n",
        "url = 'https://mymldatasets.s3.eu-de.cloud-object-storage.appdomain.cloud/el_quijote.txt'\n",
        "wget.download(url)"
      ],
      "metadata": {
        "id": "LknkgrmP51-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a60d109d-0c92-42e5-ca6d-131d821a0f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'el_quijote.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has about 1 million characters, enough to generate text convincingly."
      ],
      "metadata": {
        "id": "vTwIqkJB8HW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"el_quijote.txt\", \"r\", encoding='utf-8')\n",
        "text = f.read()\n",
        "text[:300], len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9BuFCwR7YjM",
        "outputId": "f38d6438-54f7-4a2b-b5e3-67f72d4ec3ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('DON QUIJOTE DE LA MANCHA\\nMiguel de Cervantes Saavedra\\n\\nPRIMERA PARTE\\nCAPÍTULO 1: Que trata de la condición y ejercicio del famoso hidalgo D. Quijote de la Mancha\\nEn un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, ada',\n",
              " 1038397)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "sphM8Df-7yj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To give this text to the neural network we need to transform it into numbers with which we can carry out the operations that take place in the network, this process is known as tokenization.\n",
        "\n",
        "In this case we will simply replace each character in the text with its position in the following vector of characters."
      ],
      "metadata": {
        "id": "h_N6eqN-8Xqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "all_characters = string.printable + \"ñÑáÁéÉíÍóÓúÚ¿¡\" # The last characters for Castilian Spanish are added\n",
        "all_characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bq3FGR798psL",
        "outputId": "2afd6e75-46b3-4788-d45d-afe1c0cc230a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0cñÑáÁéÉíÍóÓúÚ¿¡'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.all_characters = all_characters\n",
        "        self.n_characters = len(self.all_characters)\n",
        "\n",
        "    def text_to_seq(self, string):\n",
        "        seq = []  \n",
        "        for c in range(len(string)):\n",
        "            try:\n",
        "                seq.append(self.all_characters.index(string[c]))\n",
        "            except:\n",
        "                continue\n",
        "        return seq\n",
        "      \n",
        "    def seq_to_text(self, seq):\n",
        "        text = ''\n",
        "        for c in range(len(seq)):\n",
        "            text += self.all_characters[seq[c]]\n",
        "        return text"
      ],
      "metadata": {
        "id": "5UqHmi_d9FH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.n_characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CUYZqdf-zCu",
        "outputId": "aaed9e95-9a93-49d9-e7f9-70558737fb73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.text_to_seq('señor, ¿qué tal?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCiAPLoE-1qE",
        "outputId": "50c299ae-a5d4-4fa5-f0d0-4969b2581c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[28, 14, 100, 24, 27, 73, 94, 112, 26, 30, 104, 94, 29, 10, 21, 82]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.seq_to_text([28, 14, 100, 24, 27, 73, 94, 112, 26, 30, 104, 94, 29, 10, 21, 82])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MUU2hzj__XXU",
        "outputId": "6bbd58c1-604d-489f-a2f3-7b2fa512d73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'señor, ¿qué tal?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will tokenize all the text."
      ],
      "metadata": {
        "id": "P5YH0BZp_bZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoded = tokenizer.text_to_seq(text)"
      ],
      "metadata": {
        "id": "dW-eIvT8CBw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "lfvvmCWhCExh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We separate our text into a training set and a test set."
      ],
      "metadata": {
        "id": "aVMAEUXwCUNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = len(text_encoded) * 80 // 100\n",
        "train = text_encoded[:train_size]\n",
        "test = text_encoded[train_size:]\n",
        "\n",
        "len(train), len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vahdSBH1CjBc",
        "outputId": "ccad42cd-930b-4f2c-aee1-cf63db1fffdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(814065, 203517)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the network we need text sequences of a certain length, we use the following function"
      ],
      "metadata": {
        "id": "T1hXech7C99a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def windows(text, window_size=100):\n",
        "    start_index = 0\n",
        "    end_index = len(text) - window_size\n",
        "    text_windows = []\n",
        "    while start_index < end_index:\n",
        "        text_windows.append(text[start_index:start_index + window_size])\n",
        "        start_index += 1\n",
        "    return text_windows"
      ],
      "metadata": {
        "id": "elW6OFpPDVy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoded_windows = windows(text_encoded)"
      ],
      "metadata": {
        "id": "FqmoRSGTEEWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.seq_to_text((text_encoded_windows[0])))\n",
        "print()\n",
        "print(tokenizer.seq_to_text((text_encoded_windows[1])))\n",
        "print()\n",
        "print(tokenizer.seq_to_text((text_encoded_windows[2])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y97xd_WGEfj9",
        "outputId": "e3a69872-d64c-4580-ff1a-4a32e50ea1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DON QUIJOTE DE LA MANCHA\n",
            "Miguel de Cervantes Saavedra\n",
            "\n",
            "PRIMERA PARTE\n",
            "CAPITULO 1: Que trata de la con\n",
            "\n",
            "ON QUIJOTE DE LA MANCHA\n",
            "Miguel de Cervantes Saavedra\n",
            "\n",
            "PRIMERA PARTE\n",
            "CAPITULO 1: Que trata de la cond\n",
            "\n",
            "N QUIJOTE DE LA MANCHA\n",
            "Miguel de Cervantes Saavedra\n",
            "\n",
            "PRIMERA PARTE\n",
            "CAPITULO 1: Que trata de la condi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pytorch dataset will take care of giving us each of these phrases, using all characters except the last one as inputs to the network and the last character as the label that we will use during training."
      ],
      "metadata": {
        "id": "nzIR74k5ElQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class CharRNNDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, text_encoded_windows, train=True):\n",
        "        self.text = text_encoded_windows\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "    \n",
        "    def __getitem__(self, ix):\n",
        "        if self.train:\n",
        "            return torch.tensor(self.text[ix][:-1]), torch.tensor(self.text[ix][-1])\n",
        "        return torch.tensor(self.text[ix])"
      ],
      "metadata": {
        "id": "BKTKOEr5E9md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_encoded_windows = windows(train)\n",
        "test_text_encoded_windows = windows(test)\n",
        "\n",
        "dataset = {\n",
        "    'train': CharRNNDataset(train_text_encoded_windows),\n",
        "    'val': CharRNNDataset(test_text_encoded_windows)\n",
        "}\n",
        "\n",
        "dataloader = {\n",
        "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=512, shuffle=True, pin_memory=True),\n",
        "    'val': torch.utils.data.DataLoader(dataset['val'], batch_size=2048, shuffle=False, pin_memory=True),\n",
        "}\n",
        "\n",
        "len(dataset['train']), len(dataset['val'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCQmTY83GItl",
        "outputId": "9e551107-2e51-473c-a0e9-9ca35c1a8c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(813965, 203417)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input, output = dataset['train'][0]\n",
        "tokenizer.seq_to_text(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7o_we54TGMcN",
        "outputId": "7c919edd-af65-4844-a385-4e8b4d23bf8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DON QUIJOTE DE LA MANCHA\\nMiguel de Cervantes Saavedra\\n\\nPRIMERA PARTE\\nCAPITULO 1: Que trata de la co'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.seq_to_text([output])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Hc57A6NxGo_O",
        "outputId": "11fd1d66-12cf-4857-e3df-4854e3a4a0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings"
      ],
      "metadata": {
        "id": "lvx-AqrOKrPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although we have managed to convert our text to numbers, the neural network will still not be able to work with our data because they have to be normalized.\n",
        "\n",
        "One option may be the one-hot encoding, after all we can consider each letter as a category and our network gives us a probability distribution over all possible characters. Which will be very expensive (very large vectors) and inefficient (practically full of zeros).\n",
        "\n",
        "This is why we use \"embeddings\", an embedding is a matrix with a number of rows equal to the size of the vocabulary and a number of columns (which represent some kind of meaning) that we will decide. Unlike one-hot encoding, these vectors are dense (they can have non-zero values at any position) and these values are learned by the neural network, so that it will be able to represent the data in the best possible way to carry out homework."
      ],
      "metadata": {
        "id": "PCT4OXFeGs8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, embedding_size=128, hidden_size=256, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.encoder = torch.nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = torch.nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x, h = self.rnn(x)         \n",
        "        y = self.fc(x[:,-1,:])\n",
        "        return y"
      ],
      "metadata": {
        "id": "GbRQGsTJKwS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model will receive batches of phrases with the index of each word provided by the tokenizer, at the output we will have a probability distribution over all the possible characters for each phrase in the batch and those with the highest probability will be those that the network believes are good candidates to follow the sentence received at the entrance."
      ],
      "metadata": {
        "id": "z4qGZxZoOR_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharRNN(input_size=tokenizer.n_characters)\n",
        "outputs = model(torch.randint(0, tokenizer.n_characters, (64, 50)))\n",
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DlG6IoROiPY",
        "outputId": "ba110bb8-64fc-4e6d-d001-584343eb2c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 114])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "vJc5_asROkXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def fit(model, dataloader, epochs=10):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        bar = tqdm(dataloader['train'])\n",
        "        for batch in bar:\n",
        "            X, y = batch\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(X)\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "            bar.set_description(f\"loss {np.mean(train_loss):.5f}\")\n",
        "        bar = tqdm(dataloader['val'])\n",
        "        val_loss = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in bar:\n",
        "                X, y = batch\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                y_hat = model(X)\n",
        "                loss = criterion(y_hat, y)\n",
        "                val_loss.append(loss.item())\n",
        "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f}\")\n",
        "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f}\")\n",
        "\n",
        "def predict(model, X):\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        X = torch.tensor(X).to(device)\n",
        "        pred = model(X.unsqueeze(0))\n",
        "        return pred"
      ],
      "metadata": {
        "id": "kxFHxCaaOpbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharRNN(input_size=tokenizer.n_characters)\n",
        "fit(model, dataloader, epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKxxDWlFO0oO",
        "outputId": "e340d051-af13-4281-9ea6-83047ef0d168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.85040: 100%|██████████| 1590/1590 [03:01<00:00,  8.75it/s]\n",
            "val_loss 1.57951: 100%|██████████| 100/100 [00:15<00:00,  6.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 loss 1.85040 val_loss 1.57951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.49210: 100%|██████████| 1590/1590 [03:03<00:00,  8.68it/s]\n",
            "val_loss 1.44539: 100%|██████████| 100/100 [00:15<00:00,  6.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 loss 1.49210 val_loss 1.44539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.39133: 100%|██████████| 1590/1590 [03:02<00:00,  8.69it/s]\n",
            "val_loss 1.38293: 100%|██████████| 100/100 [00:16<00:00,  5.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 loss 1.39133 val_loss 1.38293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.33564: 100%|██████████| 1590/1590 [03:02<00:00,  8.71it/s]\n",
            "val_loss 1.34954: 100%|██████████| 100/100 [00:15<00:00,  6.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 loss 1.33564 val_loss 1.34954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.29927: 100%|██████████| 1590/1590 [03:02<00:00,  8.72it/s]\n",
            "val_loss 1.32411: 100%|██████████| 100/100 [00:15<00:00,  6.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 loss 1.29927 val_loss 1.32411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.27124: 100%|██████████| 1590/1590 [03:02<00:00,  8.72it/s]\n",
            "val_loss 1.30420: 100%|██████████| 100/100 [00:16<00:00,  5.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 loss 1.27124 val_loss 1.30420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.24906: 100%|██████████| 1590/1590 [03:00<00:00,  8.79it/s]\n",
            "val_loss 1.29014: 100%|██████████| 100/100 [00:16<00:00,  5.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 loss 1.24906 val_loss 1.29014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.23091: 100%|██████████| 1590/1590 [03:01<00:00,  8.75it/s]\n",
            "val_loss 1.28206: 100%|██████████| 100/100 [00:15<00:00,  6.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 loss 1.23091 val_loss 1.28206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.21729: 100%|██████████| 1590/1590 [03:02<00:00,  8.72it/s]\n",
            "val_loss 1.27075: 100%|██████████| 100/100 [00:15<00:00,  6.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 loss 1.21729 val_loss 1.27075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.20360: 100%|██████████| 1590/1590 [03:02<00:00,  8.70it/s]\n",
            "val_loss 1.26932: 100%|██████████| 100/100 [00:16<00:00,  5.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 loss 1.20360 val_loss 1.26932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.19139: 100%|██████████| 1590/1590 [03:01<00:00,  8.74it/s]\n",
            "val_loss 1.26210: 100%|██████████| 100/100 [00:15<00:00,  6.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 loss 1.19139 val_loss 1.26210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.17964: 100%|██████████| 1590/1590 [03:02<00:00,  8.71it/s]\n",
            "val_loss 1.25691: 100%|██████████| 100/100 [00:17<00:00,  5.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 loss 1.17964 val_loss 1.25691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.17033: 100%|██████████| 1590/1590 [03:01<00:00,  8.76it/s]\n",
            "val_loss 1.25436: 100%|██████████| 100/100 [00:17<00:00,  5.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 loss 1.17033 val_loss 1.25436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.16143: 100%|██████████| 1590/1590 [03:02<00:00,  8.73it/s]\n",
            "val_loss 1.25164: 100%|██████████| 100/100 [00:15<00:00,  6.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 loss 1.16143 val_loss 1.25164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.15374: 100%|██████████| 1590/1590 [03:02<00:00,  8.72it/s]\n",
            "val_loss 1.24838: 100%|██████████| 100/100 [00:15<00:00,  6.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 loss 1.15374 val_loss 1.24838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.14587: 100%|██████████| 1590/1590 [03:02<00:00,  8.71it/s]\n",
            "val_loss 1.25261: 100%|██████████| 100/100 [00:17<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 loss 1.14587 val_loss 1.25261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.13894: 100%|██████████| 1590/1590 [03:02<00:00,  8.73it/s]\n",
            "val_loss 1.24983: 100%|██████████| 100/100 [00:15<00:00,  6.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 loss 1.13894 val_loss 1.24983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.13158: 100%|██████████| 1590/1590 [03:02<00:00,  8.72it/s]\n",
            "val_loss 1.24687: 100%|██████████| 100/100 [00:15<00:00,  6.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 loss 1.13158 val_loss 1.24687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.12634: 100%|██████████| 1590/1590 [03:02<00:00,  8.72it/s]\n",
            "val_loss 1.24568: 100%|██████████| 100/100 [00:16<00:00,  6.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 loss 1.12634 val_loss 1.24568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.12039: 100%|██████████| 1590/1590 [03:02<00:00,  8.72it/s]\n",
            "val_loss 1.24515: 100%|██████████| 100/100 [00:17<00:00,  5.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 loss 1.12039 val_loss 1.24515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Text\n",
        "\n",
        "Once we have trained our model, we can give it a phrase to generate the next letter."
      ],
      "metadata": {
        "id": "T6D1H-1_O14F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = \"En un lugar de la mancha, \"\n",
        "X_new_encoded = tokenizer.text_to_seq(X_new)\n",
        "y_pred = predict(model, X_new_encoded)\n",
        "y_pred = torch.argmax(y_pred, axis=1)[0].item()\n",
        "tokenizer.seq_to_text([y_pred])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ERADlXbRQeVU",
        "outputId": "f065e318-5517-4620-980f-7616d1564da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'y'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can generate more letters by adding the predictions as part of the input, generating text letter by letter."
      ],
      "metadata": {
        "id": "v7gmTWcZQopG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])\n",
        "  y_pred = predict(model, X_new_encoded)\n",
        "  y_pred = torch.argmax(y_pred, axis=1)[0].item()\n",
        "  X_new += tokenizer.seq_to_text([y_pred])\n",
        "\n",
        "X_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Tw1BWKu8QpZu",
        "outputId": "53173256-6f08-443c-9d9c-4844b4c7449b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'En un lugar de la mancha, y el cura le dijo:\\n-Dejada de la mano a la mano a la mano a la mano a la cabeza y a la mano a la man'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated text can be repetitive if we simply keep the letter with the highest probability, to generate text with greater variety, it is common to randomly choose a letter from among those with the highest probability."
      ],
      "metadata": {
        "id": "umZ95JQdQ6IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp=1\n",
        "for i in range(1000):\n",
        "  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])\n",
        "  y_pred = predict(model, X_new_encoded)\n",
        "  y_pred = y_pred.view(-1).div(temp).exp()\n",
        "  top_i = torch.multinomial(y_pred, 1)[0]\n",
        "  predicted_char = tokenizer.all_characters[top_i]\n",
        "  X_new += predicted_char\n",
        "\n",
        "print(X_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAiUr25KQ8BV",
        "outputId": "3865f3f7-8963-49a7-b65a-c53e25679d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En un lugar de la mancha, y el cura le dijo:\n",
            "-Dejada de la mano a la mano a la mano a la mano a la cabeza y a la mano a la mano en tocar lo que vio su huirdico con tan cojadano que las muchas exalevian a aquella olla al campo, ni le dice que vuestro caballero; mas echo el cuerpo primero que premia lo que de su amo con la licencia que el le riques ojecio era mis desnos vieron la santa y a imaginar que por el tiempo se casaban y derramaron, que dijo el balao; porque no podria que trecho mal simple se entretendasa de las velas. Conociole Don Quijote que andumiendo mas por tus Grinas, solo quedo el tiquero, y el que era pasar del castillo\n",
            "voy tendemos en el punto a entretener en ellas, ni aun de que hayan condicido hijos largos, que esta apacientar de industria y mucho diaba, por acabar los armas fenciantes manos. A hacerlo) para que mas o Cesella, y Cardenio y las mercades y satisumas de un anadio de vuestro refratura y mal siempre de mala)), que decian que los cerpados y sombras me tuerta. Dijome que no encuentro llego Para de mucho retirueron los amores, que los golpes que era risca, que me decia.\n",
            "Yo me ayudee\n"
          ]
        }
      ]
    }
  ]
}